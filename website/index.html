<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <style>
        body {
            padding: 25px;
            width: 1000px;
            margin: auto;
            text-align: left;
            font-weight: 250;
            font-size: 14px;
            font-family: 'Open Sans', sans-serif;
            color: #121212;
        }
        h1, h2, h3, h4 {
            font-family: 'Source Sans Pro', sans-serif;
        }
        .image {
            width: 23rem;
            margin: auto;
        }
        </style>
        <title>CS 184</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
    </head>

    <body>

        <h1 align="middle">CS 184 Final Project Deliverable Report</h1>
        <h2 align="middle">Variable lens raytracing and enhanced camera features</h2>
        <!-- <br /> -->
        <!-- <h3>Members</h3> -->
        <p align="middle">
            Amy Doo, Jordan Bell, Katlyn Ho, Su Latt Phone 
        </p>
        <h2>Video</h2>
        <a href="https://drive.google.com/file/d/1bbxIjYzXVYXT-UH3GqCJkJZFn2k7LHVz/view?usp=sharing">Link to Video</a>
        <h2>Abstract</h2>
        
        <p>
            Our project adds functionality to the ray tracing engine that will allow it to render images as if the camera lens was composed of 
            physical lenses rather than an approximation of lenses. We also added a number of new camera features that model the abilities and 
            restrictions of a physical camera: the ability to autofocus on a part of an image using an iterative contrast-based algorithm that 
            uses a heuristic to identify when the image is most in focus, a technique to increase the exposure of the image using post-production, 
            and adding a togglable flash to the image.
        </p>
        

        <h2>
            Technical Approach
        </h2>

        <h2>Building a Basic Working Camera</h2>
        <h3>Lenstester</h3>
        <p>
            To build a basic working camera, ray tracing was utilized. <i>Lenstester</i> is a debugging tool that we implemented to make sure that 
            the rays refracted properly through a lens system. It is important to be able to go both forwards and backwards through the lenses, 
            as we need to be able to calculate the close focal point and infinity focal point for our lens system.
        </p>
        <h3>Lens Parameters and Sample Generation</h3>

        <p>
            Working alongside these tracing functions, we have <i>Lens</i> and <i>LensCamera</i> functions that are essential to help with setting lens parameters, setting focus depth, sampling from the back of the lens that is nearest to the sensor, and generating a ray from the sensor towards the point sampled from the back of the lens nearest to the sensor and tracing this ray through the lens.
        </p>
        <p>
            The <code>set_focus_params()</code> method calculates infinity focus, near focus, and focal length based on the thick lens approximation and uses the forward and backward ray tracing methods. 
        </p>
        <div align="center">
            <img src="./images/thick_lens.png" width="250px">
        </div>
        <p>
            Using the ray-plane intersection equation, the intersection of the resulting refracted ray through the lens and the z axis can be found to determine 
            the focus parameters.
        </p>
        <p>
            The implementation of the <code>focus_depth()</code> method allows for the user to blur or sharpen the rendered image. It computes the conjugate of a ray 
            starting at the given sensor depth and traces a ray originating from the sensor depth on the z axis through the lens using the <code>trace()</code> function implemented in Lenstester. The intersection of the resulting refracted ray with the z axis gives the final value.
        </p>
        <p>
            For the implementation of <code>back_lens_sample()</code> function, first of all, a fixed z-coordinate was calculated; the z-coordinate is fixed because the 
            function is specifically sampling a point from the back of the lens that is the closest to the sensor, so this is fixed.  Then, for the sampler, 
            at first, UniformGridSampler2D was used, because only the x-coordinate and y-coordinate were needed.  However, the mistake was realized because 
            this samples the points from a 2D grid (square) instead of a 2D circle, certain points sampled were out of bounds on a circle.  Hence,  
            UniformHemisphereSampler3D is used to sample a point in a hemisphere.  However, since the z-coordinate is already fixed, only the x-coordinate and 
            the y-coordinate are selected from the sampler to construct the function’s sampled point.
        </p>

        <p>
            For the implementation of <code>generate_ray()</code> function, the main purpose is to generate a ray from the sensor pointing toward a point on the back of the lens that is nearest to the sensor, and trace it through the lens.  First, the sensor’s position is calculated using the current screen aspect ratio and modeled 
            as a standard “full-frame” sensor.  Then, we create a ray using the sensor’s position as the origin of the ray and its direction is constructed with a 
            vector from the sensor position pointing toward the point sampled using the define <code>back_lens_sample()</code> function. Then, this ray is traced through the lens using the forward tracing function that was defined.  If the ray made it through the lens, the ray is converted into world view.  Otherwise, a ray that 
            won’t hit anything is returned.
        </p>

        <h2>Advanced Features</h2>
        <h3>Contrast-based Auto-Focus</h3>
        <p>
            We based our Auto-Focus algorithm algorithm off of the older spec, but extended it and tested different heuristics in order to get the best results. 
            The algorithm works by selecting a small portion of the scene and iteratively testing different sensor plane locations. At each location we re-render 
            the small portion, then calculate a heuristic from the image that measures the amount of contrast. After iterating over the entire range we then set 
            the sensor to the position that resulted in the highest amount of contrast.
        </p>
        <p>
            The bounds of this iteration are the infinity focal point plane and near focal plane that were determined previously, as those are the furthest the 
            sensor plane can ever move.
        </p>
        <p>
            The step size of this iteration is made to be small enough that the largest possible circle of confusion for an object that we are trying to focus 
            on is smaller than a pixel, ensuring that it can be in perfect focus. To do this we used the equation for circle of confusion from lecture: 
        </p>
        <div align="center">
            <p><code>C / A = |Z_s - Z_i| / Z_i</code></p>
        </div>

        <p>
            We plug in the height of a pixel for C, the current lens aperture size for A, and then we know that our step size should be at most <code>2 * |Z_s - Z_i|</code> 
            as this ensures that it will test within the <code>|Z_s - Z_i|</code> on either side. Solving for that we get:
        </p>
        <div align="center">
            <p>
                Step size = <code>2 * C * Z_i / A</code>
            </p>
        </div>
        <p>
            <code>Z_i</code> is unknown as that would require us to know the optimal distance for the sensor plane, but to make sure that we have a small enough step size we 
            use the smallest possible <code>Z_i</code>, the infinity focal plane. We tested different variations of this algorithm but decided anything other than iterating 
            over the whole range was not robust enough. In theory the contrast detecting heuristic should rise to a peak and then fall off afterwards, but none 
            of the heuristics we tested had smooth enough data to use any peak finding algorithms as when tested on different locations and images they had a 
            high probability to find false peaks.
        </p>

        <p> 
            We tested four different heuristics:
            <ol>
                <li>
                    A variance based heuristic that computes the variance of RGB separately and then sums them.
                </li>
                <p>The next three heuristics all first convert the image to grayscale by summing the RGB channels weighted by human perception.</p>
                <li>
                    A Sum-Modified Laplacian heuristic that detects edges by quadrupling the current pixel and subtracting the neighboring 4 pixels, 
                    summing to the total if the value is above a certain threshold
                </li>
                <li>
                    A gradient based heuristic that subtracts the pixel value below and to the left from the current pixel before summing
                </li>
                <li>
                    A similar heuristic to 1. but computing the variance altogether after converting to grayscale.
                </li>
            </ol>
        </p>
        <p>
            We decided to use 4. as it provided consistent results when using large and small images and when there was even a small amount of 
            contrast in the selection. 1 was similar but performed worse overall, 2 struggled when there was less contrast and also required precise 
            tuning, and 3 only worked when the edge was a certain orientation. Using 4 we could use an area on the order of 6x6 pixels to focus on, 
            meaning that the algorithm ran very quickly, 30 sec to 1 min depending on the lens. 
        </p>

        <h3>Flash</h3>
        <p>
            Another feature we added was flash, which can be toggled on and off with a keyboard command. In order to implement flash, we had to add 
            a light source that was at the same position and pointing in the same direction as the camera. Since the scenes we had were already bright, 
            adding flash to them wouldn’t have made an appreciable difference, so we modified an existing scene to have no light coming from any 
            place other than the flash for use in rendering images to show flash working. Figuring out how to add and modify lights in a scene was 
            mainly trial and error after skimming the COLLADA spec for a better idea of how dae files work. We tried using different types of lights 
            for the flash and found that using an area light was most realistic.
        </p>

        <h3>Auto-Exposure</h3>
        <p>
            The final feature that we implemented adds a little post-production editing directly to the scene where auto exposure can be toggled on and off. 
            When implementing this, the equation and values by which to scale the RGB pixel values took a lot of research and trial and error to find what 
            worked best. Multiplying instead of adding certain values would lead to drastically different outputs that would change things such as exposure, 
            saturation, and contrast in the image. The resources and suggestions for changing just the exposure of the image didn’t turn out as expected, 
            but we were able to find a <code>tonemap()</code> method in the given code that does edit the exposure of the RGB pixels in the sample buffer. 
            
            <div align="center">
                <code>new RGB value = (old RGB value / (avgBrightness * 2)) ^ 0.5</code>
            </div>
            <br>

            Auto-exposure takes each pixel value and scales it by the amount shown in this equation, where <code>avgBrightness</code> is the average illuminance of the pixels. This final equation is an adaptation of the code in <code>tonemap()</code> and was derived after testing of different scaling values. 
            Seeing how increasing/decreasing, adding/subtracting, multiplying/dividing, etc. changed the image helped in narrowing down the scaling equation. 
        </p>

        <h3>Other Problems Encountered</h3>
        <p>
            The first major problem we encountered occurred was when interfacing with an older version of pathtracer. Since our project is based off of an 
            old version of CS184 project, there are numerous differences between the old project spec and our current ones that required tweaking to get things setup. 
        </p>
        <p>
            The most major two differences were that the older spec used a Spectrum struct to represent light rather than a Vector3D, and that the older spec did 
            not use an alpha channel when displaying colors. We had to go through the project and fix all of these issues before it would build.
        </p>
        <p>
            There were also some struggles with building as the project only builds on Mac and Linux, but since most of our group members had a Mac, this was not a 
            major obstacle as the other members could use a VM.
        </p>
        <p>
            Another problem we had was a large amount of noise when rendering with the fisheye lens. We determined that this was because it is much more for a ray 
            to not make it all the way through the lens, as it refracted at much sharper angles. To fix this issue, whenever we call the generate_ray function we 
            continue attempting to generate a ray until one makes it through the whole lens configuration, with an upper limit as certain locations cannot generate 
            rays that pass through. The last step is to return the number of rays checked and add that to the total samples when dividing the summed returned 
            radiance, as otherwise we are not weighting the returned rays correctly.
        </p>
        <h3>Lessons Learned</h3>
        <p>
            From this project we learned about the limitations of the single lens model, and the differences between it and a multi-lens system. For autofocus, 
            we found that finding a good heuristic is a difficult task for all scenes, and we thought that it could be a possible area for Machine Learning. 
            Finally, we found that it is more challenging to implement our own ideas, but the end results are more satisfying and rewarding.
        </p>

        <h1>Results</h1>
        <p>
            We start with our four lenses, both unfocused and then autofocused. We manually moved the camera before rendering so that the dragon takes up about the same amount of space.
        </p>
        <h3>50mm Double-Gauss</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/lens_1_blur.png" width="480px" />
                  <figcaption align="middle">Unfocused</figcaption>
                </td>
                <td align="middle">
                  <img src="images/lens_1_clear.png" width="480px" />
                  <figcaption align="middle">Focused</figcaption>
                </td>
              </tr>
            </table>
        </div>
        <h3>22mm Wide-Angle</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/lens_2_blur.png" width="480px" />
                  <figcaption align="middle">Unfocused</figcaption>
                </td>
                <td align="middle">
                  <img src="images/lens_2_clear.png" width="480px" />
                  <figcaption align="middle">Focused</figcaption>
                </td>
              </tr>
            </table>
        </div>
        <h3>250mm Telephoto</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/lens_3_blur.png" width="480px" />
                  <figcaption align="middle">Unfocused</figcaption>
                </td>
                <td align="middle">
                  <img src="images/lens_3_clear.png" width="480px" />
                  <figcaption align="middle">Focused</figcaption>
                </td>
              </tr>
            </table>
        </div>
        <h3>10mm Fisheye</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/lens_4_blur.png" width="480px" />
                  <figcaption align="middle">Unfocused</figcaption>
                </td>
                <td align="middle">
                  <img src="images/lens_4_clear.png" width="480px" />
                  <figcaption align="middle">Focused</figcaption>
                </td>
              </tr>
            </table>
        </div>
        <p>
            Here are some images diplaying our flash capabilities.
        </p>
        <h3>Scene used to test flash</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/black.png" width="480px" />
                  <figcaption align="middle">CB_dragon modified to have no light</figcaption>
                </td>
              </tr>
            </table>
        </div>
        <h3>Flash with a few different lenses</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/area-light-2.png" width="480px" />
                  <figcaption align="middle">50mm Double-Gauss</figcaption>
                </td> 
                <td align="middle">
                  <img src="images/flash-lens-2.png" width="480px" />
                  <figcaption align="middle">22mm Wide-Angle</figcaption>
                </td> 
              </tr>
            </table>
        </div>
        <h3>Flash working like an actual flash</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                    <img src="images/angled-camera.png" width="480px" />
                    <figcaption align="middle">Shows that flash moves with the camera</figcaption>
                </td> 
                <td align="middle">
                  <img src="images/flash-lens-1-far.png" width="480px" />
                  <figcaption align="middle">Image is less bright when the camera is farther away</figcaption>
                </td> 
              </tr>
            </table>
        </div>

        <p> And finally here are images showing off the auto-exposure</p>
        <h3>50mm Dougle-Gauss</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/exposure1_orig.png" width="480px" />
                  <figcaption align="middle">Original</figcaption>
                </td>
                <td align="middle">
                  <img src="images/exposure1.png" width="480px" />
                  <figcaption align="middle">With exposure</figcaption>
                </td>
              </tr>
            </table>
        </div>
        <h3>22mm Wide-Angle</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/exposure2_orig.png" width="480px" />
                  <figcaption align="middle">Original</figcaption>
                </td>
                <td align="middle">
                  <img src="images/exposure2.png" width="480px" />
                  <figcaption align="middle">With exposure</figcaption>
                </td>
              </tr>
            </table>
        </div>
        <h3>250mm Telephoto</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/exposure3_orig.png" width="480px" />
                  <figcaption align="middle">Original</figcaption>
                </td>
                <td align="middle">
                  <img src="images/exposure3.png" width="480px" />
                  <figcaption align="middle">With exposure</figcaption>
                </td>
              </tr>
            </table>
        </div>
        <h3>10mm Fisheye</h3>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/exposure4_orig.png" width="480px" />
                  <figcaption align="middle">Original</figcaption>
                </td>
                <td align="middle">
                  <img src="images/exposure4.png" width="480px" />
                  <figcaption align="middle">With exposure</figcaption>
                </td>
              </tr>
            </table>
        </div>


        <h3>Flash and Auto-Exposure</h3>
        <p>
            These are images rendered with no light sources other then the flash, using auto-exposure to make them seem brighter. We can tell it is effective as the final image looks almost identical to when there is a larger light source.
        </p>
        <div align="center">
            <table style="width=100%">
              <tr>
                <td align="middle">
                  <img src="images/CBdragon_dark_dim.png" width="480px" />
                  <figcaption align="middle">Only Flash</figcaption>
                </td>
                <td align="middle">
                  <img src="images/CBdragon_dark_bright.png" width="480px" />
                  <figcaption align="middle">Flash and Auto-Exposure</figcaption>
                </td>
              </tr>
            </table>
        </div>
        <h1>References</h1>
        <ul>
            <li>Old project spec: <a href="https://cs184.eecs.berkeley.edu/sp16/article/19">https://cs184.eecs.berkeley.edu/sp16/article/19</a></li>
            <li>Paper the spec references: <a href="https://www.cs.utexas.edu/~fussell/courses/cs395t/lens.pdf">https://www.cs.utexas.edu/~fussell/courses/cs395t/lens.pdf </a> </li>
            <li>Snell's Law in 3D references: 
                <ul>
                    <li><a href="https://graphics.stanford.edu/courses/cs148-10-summer/docs/2006--degreve--reflection_refraction.pdf">https://graphics.stanford.edu/courses/cs148-10-summer/docs/2006--degreve--reflection_refraction.pdf</a></li>
                    <li><a href="https://physics.stackexchange.com/questions/435512/snells-law-in-vector-form">https://physics.stackexchange.com/questions/435512/snells-law-in-vector-form</a></li>
                </ul>
            </li>
            <li>COLLADA reference: <a href="https://www.khronos.org/collada/presentations/sailing_the_gulf/COLLADA-Chapter4.pdf">https://www.khronos.org/collada/presentations/sailing_the_gulf/COLLADA-Chapter4.pdf</a></li>
            <li>Sum-Modified Laplacian: <a href="https://www1.cs.columbia.edu/CAVE/publications/pdfs/Nayar_CVPR92.pdf">https://www1.cs.columbia.edu/CAVE/publications/pdfs/Nayar_CVPR92.pdf </a></li>
        </ul>
         
        <h2>Contributions from each team member</h2>
        <p>
            For the proposals, slides, reports, and presentations, each team member contributed and wrote about their respective part of the code.
        </p>
        <p>
            <u>Katlyn Ho</u> <br />
            She worked on the helper functions, set_focus_params() and focus_depth(), for Lens and LensCamera and auto-exposure.
        </p>
        <p>
            <u>Jordan Bell </u><br />
            Worked on setting up the original code and fixing any issues with the older spec. Helped to debug the lenstester to get it working, and did 
            the work to implement the contrast-based autofocus.
        </p>
        <p>
            <u>Amy Doo</u> <br/>
            Helped debug lenstester. Worked on flash.
        </p>
        <p>
            <u>Su Latt Phone</u><br />
            Implemented the Lens and LensCamera helper functions back_lens_sample() and generate_rays().
        </p>


    </body>
</html>